{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12184939,"sourceType":"datasetVersion","datasetId":7674224}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data-Processing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\ndf = pd.read_csv(r\"/kaggle/input/clotho-dataset/clotho_captions_development.csv\")\naudio_folder = r'/kaggle/input/clotho-dataset/clotho_audio_development'\n\ndef find_audio_path(file_name):\n    path = os.path.join(audio_folder, file_name)\n    if os.path.exists(path):\n        return path\n    print(\"not find\",path)\n\ndf['audio_path'] = df['file_name'].apply(find_audio_path)\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:35.698818Z","iopub.execute_input":"2025-07-22T00:58:35.699073Z","iopub.status.idle":"2025-07-22T00:58:40.907905Z","shell.execute_reply.started":"2025-07-22T00:58:35.699055Z","shell.execute_reply":"2025-07-22T00:58:40.907263Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                    file_name  \\\n0   DistortedAMRadionoise.wav   \n1  PaperParchmentRustling.wav   \n2     03WhalesSlowingDown.wav   \n3    Ropetiedtoboatinport.wav   \n4            carpenterbee.wav   \n\n                                           caption_1  \\\n0        A muddled noise of broken channel of the TV   \n1           A person is turning a map over and over.   \n2  Several barnyard animals mooing in a barn whil...   \n3  An office chair is squeaking as someone bends ...   \n4  A flying bee is buzzing loudly around an objec...   \n\n                                           caption_2  \\\n0     A television blares the rhythm of a static TV.   \n1  A person is very carefully rapping a gift for ...   \n2  The vocalization of several whales, along with...   \n3  Popping and squeaking gradually tapers off to ...   \n4  An annoying fly is buzzing loudly and consiste...   \n\n                                           caption_3  \\\n0    Loud television static dips in and out of focus   \n1  A person is very carefully wrapping a gift for...   \n2  Underwater, large numbers of shrimp clicking a...   \n3  Someone is opening a creaky door slowly while ...   \n4  An insect buzzing in the foreground as birds c...   \n\n                                           caption_4  \\\n0  The loud buzz of static constantly changes pit...   \n1  He sighed as he turned the pages of the book, ...   \n2  Whales sing to one another over the flowing wa...   \n3  Squeaking and popping followed by gradual popp...   \n4  An insect trapped in a spider web struggles, b...   \n\n                                           caption_5  \\\n0  heavy static and the beginnings of a signal on...   \n1  papers are being turned, stopped, then turned ...   \n2  wales sing to one another with water flowing i...   \n3  an office chair is squeaking as someone leans ...   \n4  Outdoors, insect trapped in a spider web and t...   \n\n                                          audio_path  \n0  /kaggle/input/clotho-dataset/clotho_audio_deve...  \n1  /kaggle/input/clotho-dataset/clotho_audio_deve...  \n2  /kaggle/input/clotho-dataset/clotho_audio_deve...  \n3  /kaggle/input/clotho-dataset/clotho_audio_deve...  \n4  /kaggle/input/clotho-dataset/clotho_audio_deve...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>caption_1</th>\n      <th>caption_2</th>\n      <th>caption_3</th>\n      <th>caption_4</th>\n      <th>caption_5</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DistortedAMRadionoise.wav</td>\n      <td>A muddled noise of broken channel of the TV</td>\n      <td>A television blares the rhythm of a static TV.</td>\n      <td>Loud television static dips in and out of focus</td>\n      <td>The loud buzz of static constantly changes pit...</td>\n      <td>heavy static and the beginnings of a signal on...</td>\n      <td>/kaggle/input/clotho-dataset/clotho_audio_deve...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PaperParchmentRustling.wav</td>\n      <td>A person is turning a map over and over.</td>\n      <td>A person is very carefully rapping a gift for ...</td>\n      <td>A person is very carefully wrapping a gift for...</td>\n      <td>He sighed as he turned the pages of the book, ...</td>\n      <td>papers are being turned, stopped, then turned ...</td>\n      <td>/kaggle/input/clotho-dataset/clotho_audio_deve...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>03WhalesSlowingDown.wav</td>\n      <td>Several barnyard animals mooing in a barn whil...</td>\n      <td>The vocalization of several whales, along with...</td>\n      <td>Underwater, large numbers of shrimp clicking a...</td>\n      <td>Whales sing to one another over the flowing wa...</td>\n      <td>wales sing to one another with water flowing i...</td>\n      <td>/kaggle/input/clotho-dataset/clotho_audio_deve...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ropetiedtoboatinport.wav</td>\n      <td>An office chair is squeaking as someone bends ...</td>\n      <td>Popping and squeaking gradually tapers off to ...</td>\n      <td>Someone is opening a creaky door slowly while ...</td>\n      <td>Squeaking and popping followed by gradual popp...</td>\n      <td>an office chair is squeaking as someone leans ...</td>\n      <td>/kaggle/input/clotho-dataset/clotho_audio_deve...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>carpenterbee.wav</td>\n      <td>A flying bee is buzzing loudly around an objec...</td>\n      <td>An annoying fly is buzzing loudly and consiste...</td>\n      <td>An insect buzzing in the foreground as birds c...</td>\n      <td>An insect trapped in a spider web struggles, b...</td>\n      <td>Outdoors, insect trapped in a spider web and t...</td>\n      <td>/kaggle/input/clotho-dataset/clotho_audio_deve...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"# Pre-Processing Data","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nimport torchaudio.transforms as T\nimport torchaudio.functional as F\nimport random\n\nclass AudioCaptionDataset(Dataset):\n    def __init__(self, audio_paths, captions, tokenizer, sr=32000, n_mels=128, max_len=30, augment = True):\n        self.audio_paths = audio_paths\n        self.captions = captions\n        self.tokenizer = tokenizer\n        self.sr = sr #16000 # 42100\n        self.n_mels = n_mels\n        self.max_len = max_len\n        self.augment = augment\n        self.mel_transform = T.MelSpectrogram(\n            sample_rate=self.sr,\n            n_fft=1024, # 4096/ 2048\n            win_length=1024, #4096  2048\n            hop_length= 320, #512 256\n            n_mels=self.n_mels,\n            f_min=20,\n            f_max=self.sr // 2\n        )\n        self.spec_augment = torch.nn.Sequential(\n            T.FrequencyMasking(freq_mask_param=15),\n            T.TimeMasking(time_mask_param=50)\n        )\n        \n    # def add_noise(self, waveform, noise_level=0.005):\n    #     noise = torch.randn_like(waveform) * noise_level\n    #     return waveform + noise\n\n    # def pitch_shift(self, waveform, n_steps=2):\n    #     return F.pitch_shift(waveform, self.sr, n_steps)\n\n    # def time_stretch(self, waveform, rate):\n    #     # Chuyển về spectrogram để time-stretch không cần sox\n    #     spectrogram = T.Spectrogram()(waveform)\n    #     stretcher = T.TimeStretch(hop_length=None, n_freq=spectrogram.shape[1])\n    #     stretched_spec = stretcher(spectrogram, rate)\n    #     # Convert ngược lại waveform nếu cần\n        \n    # def waveform_augment(self, waveform):\n    #     if random.random() < 0.5:\n    #         waveform = self.add_noise(waveform, noise_level=random.uniform(0.003, 0.008))\n    #     if random.random() < 0.3:\n    #         waveform = self.pitch_shift(waveform, n_steps=random.randint(-1, 1))\n    #     if random.random() < 0.3:\n    #         waveform = self.time_stretch(waveform, rate=random.uniform(0.95, 1.05))\n    #     return waveform\n\n    def __getitem__(self, idx):\n        # Load audio\n        waveform, orig_sr = torchaudio.load(self.audio_paths[idx])\n        if orig_sr != self.sr:\n            waveform = T.Resample(orig_sr, self.sr)(waveform)\n            \n        max_len_audio = self.sr * 25  # 10 giây\n        waveform = waveform[:, :max_len_audio]\n        if waveform.shape[1] < max_len_audio:\n            pad = max_len_audio - waveform.shape[1]\n            waveform = torch.nn.functional.pad(waveform, (0, pad))\n            \n        # if self.augment:\n        #     waveform = self.waveform_augment(waveform)\n            \n        mel_spec = self.mel_transform(waveform)\n        log_mel = torch.log1p(mel_spec)\n        #log_mel = (log_mel - log_mel.mean()) / log_mel.std()  # Normalize\n        log_mel = (log_mel - log_mel.mean(dim=(-2, -1), keepdim=True)) / (log_mel.std(dim=(-2, -1), keepdim=True) + 1e-9)\n        log_mel = log_mel.repeat(3, 1, 1) #mel_spec only 1 channel but resnet have 3 channel\n        \n        if self.augment:\n            log_mel = self.spec_augment(log_mel)     \n            \n        # Tokenize caption\n        caption_text = self.captions[idx]\n        caption_tokens = self.tokenizer.encode(caption_text, max_len=self.max_len)\n        caption_tensor = torch.tensor(caption_tokens, dtype=torch.long)\n\n        return log_mel, caption_tensor  # Now return tensor #log_mel.squeeze(0)\n\n    def __len__(self):\n        return len(self.audio_paths)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:40.909255Z","iopub.execute_input":"2025-07-22T00:58:40.909667Z","iopub.status.idle":"2025-07-22T00:58:43.394314Z","shell.execute_reply.started":"2025-07-22T00:58:40.909648Z","shell.execute_reply":"2025-07-22T00:58:43.393496Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class SimpleTokenizer:\n    def __init__(self, texts, min_freq=1):\n        from collections import Counter\n\n        self.special_tokens = ['<pad>', '<bos>', '<eos>', '<unk>']\n        counter = Counter()\n\n        for text in texts:\n            words = text.lower().strip().split()\n            counter.update(words)\n\n        self.word2idx = {token: idx for idx, token in enumerate(self.special_tokens)}\n        for word, freq in counter.items():\n            if freq >= min_freq and word not in self.word2idx:\n                self.word2idx[word] = len(self.word2idx)\n\n        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n        self.vocab_size = len(self.word2idx)\n        self.pad_token_id = self.word2idx['<pad>']\n\n    def encode(self, text, max_len=30):\n        tokens = [self.word2idx.get(word, self.word2idx['<unk>']) for word in text.lower().strip().split()]\n        tokens = [self.word2idx['<bos>']] + tokens + [self.word2idx['<eos>']]\n        if len(tokens) < max_len:\n            tokens += [self.word2idx['<pad>']] * (max_len - len(tokens))\n        else:\n            tokens = tokens[:max_len]\n        return tokens\n\n    def decode(self, tokens):\n        words = [self.idx2word.get(idx, '<unk>') for idx in tokens]\n        return ' '.join(words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:43.395190Z","iopub.execute_input":"2025-07-22T00:58:43.395763Z","iopub.status.idle":"2025-07-22T00:58:43.402886Z","shell.execute_reply.started":"2025-07-22T00:58:43.395741Z","shell.execute_reply":"2025-07-22T00:58:43.402242Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Resize length caption","metadata":{}},{"cell_type":"code","source":"# # import torch.nn.functional as F\n\n# # def collate_fn(batch):\n# #     # batch: list of tuples (mel, caption)\n# #     mels, captions = zip(*batch)\n    \n# #     # Padding audio (log_mel)\n# #     max_len = max(mel.shape[-1] for mel in mels)  # lấy max time_steps trong batch\n\n# #     padded_mels = []\n# #     for mel in mels:\n# #         pad_size = max_len - mel.shape[-1]\n# #         padded_mel = F.pad(mel, (0, pad_size))  # Pad cuối chiều time_steps\n# #         padded_mels.append(padded_mel)\n\n# #     mels_tensor = torch.stack(padded_mels)\n# #     captions_tensor = torch.stack(captions)\n\n# #     return mels_tensor, captions_tensor\n\n# from torch.nn.utils.rnn import pad_sequence\n\n# def collate_fn(batch):\n#     audios, captions = zip(*batch)\n\n#     audios = torch.stack(audios)\n\n#     # Pad captions về cùng độ dài\n#     captions = [torch.tensor(cap, dtype=torch.long) for cap in captions]\n#     captions_padded = pad_sequence(captions, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n#     return audios, captions_padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:43.404501Z","iopub.execute_input":"2025-07-22T00:58:43.404704Z","iopub.status.idle":"2025-07-22T00:58:43.418683Z","shell.execute_reply.started":"2025-07-22T00:58:43.404688Z","shell.execute_reply":"2025-07-22T00:58:43.417956Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata = []\nfor _, row in df.iterrows():\n    audio_path = row[\"audio_path\"]\n    captions = [row[f\"caption_{i}\"] for i in range(1, 6)]\n\n    for caption in captions:\n        data.append((audio_path, caption))\naudio_paths, captions = zip(*data)\n\ntrain_audio_paths, val_audio_paths, train_captions, val_captions = train_test_split(audio_paths, captions, \n                                                                                    test_size=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:43.419446Z","iopub.execute_input":"2025-07-22T00:58:43.419914Z","iopub.status.idle":"2025-07-22T00:58:44.277700Z","shell.execute_reply.started":"2025-07-22T00:58:43.419885Z","shell.execute_reply":"2025-07-22T00:58:44.277099Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"all_captions = list(train_captions) + list(val_captions)\ntokenizer = SimpleTokenizer(all_captions)\n\ntrain_dataset = AudioCaptionDataset(train_audio_paths, train_captions, tokenizer, augment = True)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4) #, collate_fn=collate_fn\n\nval_dataset = AudioCaptionDataset(val_audio_paths, val_captions, tokenizer, augment = False)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4) #, collate_fn=collate_fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:44.278515Z","iopub.execute_input":"2025-07-22T00:58:44.278924Z","iopub.status.idle":"2025-07-22T00:58:44.413345Z","shell.execute_reply.started":"2025-07-22T00:58:44.278904Z","shell.execute_reply":"2025-07-22T00:58:44.412742Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Early Stopping","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Save model when validation loss decreases.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} → {val_loss:.6f}). Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:44.414113Z","iopub.execute_input":"2025-07-22T00:58:44.414354Z","iopub.status.idle":"2025-07-22T00:58:44.422028Z","shell.execute_reply.started":"2025-07-22T00:58:44.414332Z","shell.execute_reply":"2025-07-22T00:58:44.421342Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Encoder","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn as nn\n\n# class AudioEncoder_resnet50(nn.Module):\n#     def __init__(self, output_dim=512):  # output_dim bạn vẫn có thể muốn giảm sau ResNet\n#         super().__init__()\n#         resnet = models.resnet50(pretrained=True)\n#         resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  # chỉnh cho 3-channel log-mel\n#         modules = list(resnet.children())[:-1]  # bỏ fc\n#         self.resnet = nn.Sequential(*modules)\n#         self.fc = nn.Linear(2048, output_dim)  # map từ 2048 → output_dim\n\n#     def forward(self, x):\n#         x = self.resnet(x)\n#         x = x.view(x.size(0), -1)\n#         x = self.fc(x)\n#         return x\n\nclass AudioEncoder_50(nn.Module):\n    def __init__(self, output_dim=2048):\n        super().__init__()\n        resnet = models.resnet50(pretrained=True)\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.fc = nn.Linear(2048, output_dim)\n\n    def forward(self, x):\n        x = self.resnet(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nimport torchvision.models as models\nfrom torch import nn\n\nclass AudioEncoder_18(nn.Module):\n    def __init__(self, output_dim=512):\n        super().__init__()\n        resnet = models.resnet18(pretrained=True)\n        self.features = nn.Sequential(*list(resnet.children())[:-2])  # Bỏ FC\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, output_dim)\n        #self.projection = nn.Linear(512, 256) \n\n    def forward(self, x):\n        # Input x: (batch_size, 1, n_mels, time) → thêm 1 channel\n        x = self.features(x)\n        x = self.pool(x).squeeze(-1).squeeze(-1)\n        x = self.fc(x)\n        return x  # (batch_size, output_dim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:44.422845Z","iopub.execute_input":"2025-07-22T00:58:44.423058Z","iopub.status.idle":"2025-07-22T00:58:48.055556Z","shell.execute_reply.started":"2025-07-22T00:58:44.423042Z","shell.execute_reply":"2025-07-22T00:58:48.054922Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Decoder LSTM","metadata":{}},{"cell_type":"code","source":"class CaptionDecoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, decoder_dim, hidden_dim, num_layers=1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim + decoder_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, features, captions):\n        # captions: (batch_size, seq_len)\n        embeddings = self.embed(captions)\n        \n        features_expanded = features.unsqueeze(1).repeat(1, embeddings.size(1), 1)\n        inputs = torch.cat((features_expanded, embeddings), dim=2)\n        #inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n        \n        hiddens, _ = self.lstm(inputs)\n        outputs = self.fc(hiddens)\n        return outputs  # (batch_size, seq_len+1, vocab_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:48.056251Z","iopub.execute_input":"2025-07-22T00:58:48.056618Z","iopub.status.idle":"2025-07-22T00:58:48.061979Z","shell.execute_reply.started":"2025-07-22T00:58:48.056602Z","shell.execute_reply":"2025-07-22T00:58:48.061245Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Combined Encode + Decoder","metadata":{}},{"cell_type":"code","source":"class AudioCaptioningModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, audio_spec, captions):\n        features = self.encoder(audio_spec)\n        outputs = self.decoder(features, captions)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:48.064928Z","iopub.execute_input":"2025-07-22T00:58:48.065185Z","iopub.status.idle":"2025-07-22T00:58:48.080227Z","shell.execute_reply.started":"2025-07-22T00:58:48.065169Z","shell.execute_reply":"2025-07-22T00:58:48.079647Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Ver2 combine Lstm Attention","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torchvision.models as models\n\n# class AudioEncoder_Attention(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         resnet = models.resnet50(pretrained=True)\n#         modules = list(resnet.children())[:-2]  # Bỏ global avgpool + fc\n#         self.resnet = nn.Sequential(*modules)\n#         self.adaptive_pool = nn.AdaptiveAvgPool2d((10, 10))  # Có thể thêm pooling nhẹ nếu cần giới hạn time_steps\n\n#     def forward(self, x):\n#         x = self.resnet(x)  # (batch_size, 2048, H, W), thường H=W=~7\n#         x = self.adaptive_pool(x)  # (batch_size, 2048, 10, 10) để giảm kích thước sequence\n#         x = x.permute(0, 2, 3, 1)  # (batch_size, 10, 10, 2048)\n#         x = x.view(x.size(0), -1, 2048)  # (batch_size, 100, 2048) --> sequence cho attention\n#         return x  # output shape: (batch_size, time_steps, encoder_dim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:48.081183Z","iopub.execute_input":"2025-07-22T00:58:48.081385Z","iopub.status.idle":"2025-07-22T00:58:48.090184Z","shell.execute_reply.started":"2025-07-22T00:58:48.081370Z","shell.execute_reply":"2025-07-22T00:58:48.089475Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class BahdanauAttention(nn.Module):\n#     def __init__(self, encoder_dim, decoder_dim, attention_dim):\n#         super(BahdanauAttention, self).__init__()\n#         self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n#         self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n#         self.full_att = nn.Linear(attention_dim, 1)\n\n#     def forward(self, encoder_outputs, decoder_hidden):\n#         # encoder_outputs: (batch_size, time_steps, encoder_dim)\n#         # decoder_hidden: (batch_size, decoder_dim)\n\n#         # Add time dimension to decoder hidden\n#         decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch_size, 1, decoder_dim)\n        \n#         att1 = self.encoder_att(encoder_outputs)  # (batch_size, time_steps, attention_dim)\n#         att2 = self.decoder_att(decoder_hidden)#.unsqueeze(1)  # (batch_size, 1, attention_dim)\n        \n#         att = torch.tanh(att1 + att2)  # broadcast add\n#         e = self.full_att(att).squeeze(2)  # (batch_size, time_steps)\n        \n#         alpha = F.softmax(e, dim=1)  # attention weights (batch_size, time_steps)\n        \n#         context = (encoder_outputs * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n        \n#         return context, alpha\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:48.090957Z","iopub.execute_input":"2025-07-22T00:58:48.091197Z","iopub.status.idle":"2025-07-22T00:58:48.103286Z","shell.execute_reply.started":"2025-07-22T00:58:48.091174Z","shell.execute_reply":"2025-07-22T00:58:48.102551Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# class CaptionDecoder_Attention(nn.Module):\n#     def __init__(self, vocab_size, embed_dim, decoder_dim, hidden_dim, attention_module, num_layers=1):\n#         super().__init__()\n#         self.embed = nn.Embedding(vocab_size, embed_dim)\n#         self.lstm = nn.LSTM(embed_dim + decoder_dim, hidden_dim, num_layers, batch_first=True)\n#         self.fc = nn.Linear(hidden_dim, vocab_size)\n#         self.attention = attention_module  # <-- attention module đã định nghĩa ở trên\n        \n#     def forward(self, encoder_outputs, captions):\n#         batch_size, seq_len = captions.shape\n#         embeddings = self.embed(captions)  # (batch_size, seq_len, embed_dim)\n        \n#         h = torch.zeros(1, batch_size, self.lstm.hidden_size).to(captions.device)\n#         c = torch.zeros(1, batch_size, self.lstm.hidden_size).to(captions.device)\n\n#         outputs = []\n#         for t in range(seq_len):\n#             embedding_t = embeddings[:, t, :]  # (batch, embed_dim)\n\n#             if t == 0:\n#                 # Khởi tạo attention bằng 0\n#                 context = torch.zeros(batch_size, encoder_outputs.shape[2]).to(captions.device)\n#             else:\n#                 # decoder_hidden = h[-1] \n#                 # context, _ = self.attention(encoder_outputs, decoder_hidden)\n#                 context, _ = self.attention(encoder_outputs, h.squeeze(0))  # (batch, encoder_dim)\n\n#             lstm_input = torch.cat((embedding_t, context), dim=1).unsqueeze(1)\n#             output, (h, c) = self.lstm(lstm_input, (h, c))\n#             output_vocab = self.fc(output.squeeze(1))\n#             outputs.append(output_vocab)\n\n#         outputs = torch.stack(outputs, dim=1)  # (batch_size, seq_len, vocab_size)\n#         return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:48.104074Z","iopub.execute_input":"2025-07-22T00:58:48.104325Z","iopub.status.idle":"2025-07-22T00:58:48.115348Z","shell.execute_reply.started":"2025-07-22T00:58:48.104283Z","shell.execute_reply":"2025-07-22T00:58:48.114792Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Tranning Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom transformers import get_scheduler\n\ndef train_one_epoch(model, train_loader, optimizer, criterion, device, lr_scheduler):\n    model.train()\n    total_loss, total_samples = 0, 0\n    loop = tqdm(train_loader, leave=True)\n    for mel, captions in loop:\n        mel = mel.to(device)              \n        captions = captions.to(device)  \n\n        optimizer.zero_grad()\n        outputs = model(mel, captions[:, :-1])  \n        loss = criterion(outputs.reshape(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        lr_scheduler.step()\n\n        batch_size = mel.size(0)  \n        total_loss += loss.item() * batch_size  \n        total_samples += batch_size\n\n    avg_loss = total_loss / total_samples\n    return avg_loss\n\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    total_loss, total_samples = 0, 0\n    with torch.no_grad():\n        loop = tqdm(val_loader, leave=True)\n        for mel, captions in loop:\n            mel = mel.to(device)              \n            captions = captions.to(device)  \n        \n            outputs = model(mel, captions[:, :-1])   # Teacher Forcing: input là caption trừ token cuối\n            loss = criterion(outputs.reshape(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))     \n            \n            batch_size = mel.size(0)  # số sample trong batch\n            total_loss += loss.item() * batch_size  # cộng dồn tổng loss theo sample\n            total_samples += batch_size\n            \n    avg_loss = total_loss / total_samples\n    return avg_loss ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:48.116069Z","iopub.execute_input":"2025-07-22T00:58:48.116352Z","iopub.status.idle":"2025-07-22T00:58:49.081060Z","shell.execute_reply.started":"2025-07-22T00:58:48.116328Z","shell.execute_reply":"2025-07-22T00:58:49.080483Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size  \nencoder = AudioEncoder_50(output_dim=2048)\n#decoder = CaptionDecoder(vocab_size, embed_dim=256, encoder_dim=512, hidden_dim=512)\ndecoder = CaptionDecoder(vocab_size, embed_dim=512, decoder_dim=2048, hidden_dim=1024)\n\n# encoder = AudioEncoder_Attention()\n# attention = BahdanauAttention(encoder_dim=2048, decoder_dim=1024, attention_dim = 512)\n# decoder = CaptionDecoder_Attention(vocab_size, embed_dim=512, decoder_dim=2048, hidden_dim=1024, attention_module=attention)\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = AudioCaptioningModel(encoder, decoder)\nmodel = nn.DataParallel(model).to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=2e-4)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\nnum_epochs = 30\nnum_training_steps = num_epochs * len(train_loader)\nlr_scheduler = get_scheduler(\n    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\nearly_stopper = EarlyStopping(patience=3, verbose=True)\n\n\nfor epoch in range(num_epochs):\n    train_loss = train_one_epoch(model, train_loader,  optimizer, criterion, device, lr_scheduler)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n\n    val_loss = validate(model, val_loader, criterion, device)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}\")\n    \n    early_stopper(val_loss, model)\n    if early_stopper.early_stop:\n        print(\"Early stopping\")\n        break\n\ntorch.save(model.state_dict(), \"audio_caption_model_for_inference.pth\")\nprint(\"Model saved for inference!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T00:58:49.081919Z","iopub.execute_input":"2025-07-22T00:58:49.082337Z","iopub.status.idle":"2025-07-22T05:01:03.888583Z","shell.execute_reply.started":"2025-07-22T00:58:49.082291Z","shell.execute_reply":"2025-07-22T05:01:03.887459Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 173MB/s]\n100%|██████████| 540/540 [12:14<00:00,  1.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/30], Train Loss: 5.2195\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/30], Val Loss: 5.0863\nValidation loss decreased (inf → 5.086333). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:43<00:00,  1.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/30], Train Loss: 4.2925\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:13<00:00,  1.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/30], Val Loss: 4.1916\nValidation loss decreased (5.086333 → 4.191587). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:27<00:00,  1.38s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/30], Train Loss: 3.8754\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:04<00:00,  1.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/30], Val Loss: 3.9509\nValidation loss decreased (4.191587 → 3.950950). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/30], Train Loss: 3.5677\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/30], Val Loss: 3.7890\nValidation loss decreased (3.950950 → 3.789046). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/30], Train Loss: 3.3160\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/30], Val Loss: 3.7219\nValidation loss decreased (3.789046 → 3.721883). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/30], Train Loss: 3.0913\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/30], Val Loss: 3.6243\nValidation loss decreased (3.721883 → 3.624317). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/30], Train Loss: 2.8848\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/30], Val Loss: 3.5715\nValidation loss decreased (3.624317 → 3.571480). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/30], Train Loss: 2.6953\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:04<00:00,  1.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/30], Val Loss: 3.6524\nEarlyStopping counter: 1 out of 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/30], Train Loss: 2.5191\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:06<00:00,  1.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/30], Val Loss: 3.4974\nValidation loss decreased (3.571480 → 3.497426). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:18<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/30], Train Loss: 2.3572\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:04<00:00,  1.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/30], Val Loss: 3.4769\nValidation loss decreased (3.497426 → 3.476945). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/30], Train Loss: 2.2083\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/30], Val Loss: 3.4776\nEarlyStopping counter: 1 out of 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/30], Train Loss: 2.0695\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/30], Val Loss: 3.4667\nValidation loss decreased (3.476945 → 3.466704). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/30], Train Loss: 1.9446\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/30], Val Loss: 3.4638\nValidation loss decreased (3.466704 → 3.463775). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:18<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/30], Train Loss: 1.8284\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/30], Val Loss: 3.4567\nValidation loss decreased (3.463775 → 3.456737). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/30], Train Loss: 1.7236\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/30], Val Loss: 3.4544\nValidation loss decreased (3.456737 → 3.454409). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:19<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/30], Train Loss: 1.6265\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/30], Val Loss: 3.5048\nEarlyStopping counter: 1 out of 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:20<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/30], Train Loss: 1.5413\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/30], Val Loss: 3.4644\nEarlyStopping counter: 2 out of 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 540/540 [12:20<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/30], Train Loss: 1.4574\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/30], Val Loss: 3.4669\nEarlyStopping counter: 3 out of 3\nEarly stopping\nModel saved for inference!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"torch.save(model.state_dict(), \"audio_caption_model_for_inference.pth\")\nprint(\"Model saved for inference!\")\n# Lưu token như này để test model local\ntorch.save(tokenizer, \"tokenizer.pt\")\nprint(\"Tokenizer saved for inference!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T05:01:03.890261Z","iopub.execute_input":"2025-07-22T05:01:03.890597Z","iopub.status.idle":"2025-07-22T05:01:04.529438Z","shell.execute_reply.started":"2025-07-22T05:01:03.890568Z","shell.execute_reply":"2025-07-22T05:01:04.528442Z"}},"outputs":[{"name":"stdout","text":"Model saved for inference!\nTokenizer saved for inference!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Lưu token này để deploy tiện hơn\nimport json\n\nwith open(\"word2idx.json\", \"w\") as f:\n    json.dump(tokenizer.word2idx, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T05:01:04.531932Z","iopub.execute_input":"2025-07-22T05:01:04.532500Z","iopub.status.idle":"2025-07-22T05:01:04.543125Z","shell.execute_reply.started":"2025-07-22T05:01:04.532474Z","shell.execute_reply":"2025-07-22T05:01:04.542273Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# import pickle\n\n# with open('tokenizer.pkl', 'wb') as f:\n#     pickle.dump(tokenizer, f)\n\n# with open('tokenizer.pkl', 'rb') as f:\n#     tokenizer = pickle.load(f)\n\n# # tokenizer là instance của một class kế thừa nn.Module hoặc có hỗ trợ state_dict\n# torch.save(tokenizer.state_dict(), \"tokenizer.pt\")\n\n# from model import SimpleTokenizer  # phải có class này định nghĩa đúng\n\n# # 1. Tạo lại tokenizer đúng cấu trúc ban đầu (nhưng chưa có weights)\n# tokenizer = SimpleTokenizer(...)  # truyền các config nếu cần\n\n# # 2. Load weights\n# tokenizer.load_state_dict(torch.load(\"tokenizer.pt\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save word2idx khi muốn mang đi deploy ở chõ khác tải cái token này\n# with open('word2idx.pkl', 'wb') as f:\n#     pickle.dump(tokenizer.word2idx, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Model","metadata":{}},{"cell_type":"code","source":"import torchaudio\nimport torchaudio.transforms as T\nimport torch.nn.functional as F\n\ndef preprocess_audio(audio_path, sr=16000, n_mels=64, max_audio_len=25):\n    waveform, orig_sr = torchaudio.load(audio_path)\n    if orig_sr != sr:\n        waveform = T.Resample(orig_sr, sr)(waveform)\n    \n    max_len_audio = sr * max_audio_len\n    waveform = waveform[:, :max_len_audio]\n    if waveform.shape[1] < max_len_audio:\n        pad = max_len_audio - waveform.shape[1]\n        waveform = F.pad(waveform, (0, pad))\n\n    mel_transform = T.MelSpectrogram(sample_rate=sr, n_mels=n_mels)\n    mel_spec = mel_transform(waveform)\n    log_mel = torch.log1p(mel_spec)\n    log_mel = (log_mel - log_mel.mean()) / log_mel.std()\n    log_mel = log_mel.repeat(3, 1, 1)  # (3, n_mels, time)\n    \n    return log_mel\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(model, mel, tokenizer, max_len=20):\n    model.eval()\n    device = next(model.parameters()).device\n    \n    with torch.no_grad():\n        features = model.encoder(mel.to(device))\n        \n        # Bắt đầu với token <SOS>\n        input_word = torch.tensor([[tokenizer.word2idx['<bos>']]]).to(device)\n        captions = []\n        hidden = None\n\n        for _ in range(max_len):\n            embedding = model.decoder.embed(input_word)\n            lstm_input = torch.cat((features.unsqueeze(1), embedding), dim=2)\n            output, hidden = model.decoder.lstm(lstm_input, hidden)\n            output_vocab = model.decoder.fc(output.squeeze(1))\n            predicted = output_vocab.argmax(1)\n            \n            word = tokenizer.idx2word[predicted.item()]\n            if word == '<eos>':\n                break\n            captions.append(word)\n            input_word = predicted.unsqueeze(0)\n    \n    return ' '.join(captions)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load raw state_dict\nstate_dict = torch.load(\"/kaggle/working/audio_caption_model_for_inference.pth\", map_location=device)\n\n# Hàm để xóa 'module.' nếu có\ndef remove_module_prefix(state_dict):\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith(\"module.\"):\n            new_key = k[7:]  # bỏ 'module.' đi\n        else:\n            new_key = k\n        new_state_dict[new_key] = v\n    return new_state_dict\n\nnew_state_dict = remove_module_prefix(state_dict)\n\nmodel.load_state_dict(new_state_dict)\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mel = preprocess_audio(\"/kaggle/input/clotho-dataset/clotho_audio_development/0111Ambulance.wav\") \n# mel = mel.unsqueeze(0).to(device)  # thêm batch dimension\n\n# with torch.no_grad():\n#     features = model.encoder(mel)\n#     caption = generate_caption(model.decoder, features, tokenizer)\n#     print(caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mel = preprocess_audio(\"/kaggle/input/clotho-dataset/clotho_audio_development/00264hillcreek1.wav\")\nmel = mel.unsqueeze(0).to(device)  # thêm batch dimension\ncaption = generate_caption(model, mel, tokenizer)\nprint(\"Generated Caption:\", caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}