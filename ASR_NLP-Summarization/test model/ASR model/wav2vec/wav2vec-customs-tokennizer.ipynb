{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12763173,"sourceType":"datasetVersion","datasetId":8068415}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data-Processing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/fosd-vietnamese/transcript.csv\", encoding=\"utf-8\")\ndf[\"file_name\"] = df[\"file_name\"].str.replace(\".mp3\", \".wav\", regex=False)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:49:44.630231Z","iopub.execute_input":"2025-08-27T08:49:44.630481Z","iopub.status.idle":"2025-08-27T08:49:45.072509Z","shell.execute_reply.started":"2025-08-27T08:49:44.630458Z","shell.execute_reply":"2025-08-27T08:49:45.071652Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                  file_name  \\\n0  FPTOpenSpeechData_Set001_V0.1_000001.wav   \n1  FPTOpenSpeechData_Set001_V0.1_000002.wav   \n2  FPTOpenSpeechData_Set001_V0.1_000003.wav   \n3  FPTOpenSpeechData_Set001_V0.1_000004.wav   \n4  FPTOpenSpeechData_Set001_V0.1_000005.wav   \n\n                                             caption              tag  \n0                                         cách để đi  0.00000-1.27298  \n1  họ đã xét nghiệm máu cho cheng nhưng mọi thứ v...  0.00000-3.79298  \n2                           anh có thể gọi tôi không  0.00000-2.52098  \n3                  có rất nhiều yếu tố may rủi ở đây  0.00000-3.43298  \n4                                 ai là chúa nói dối  0.00000-3.93698  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>caption</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000001.wav</td>\n      <td>cách để đi</td>\n      <td>0.00000-1.27298</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000002.wav</td>\n      <td>họ đã xét nghiệm máu cho cheng nhưng mọi thứ v...</td>\n      <td>0.00000-3.79298</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000003.wav</td>\n      <td>anh có thể gọi tôi không</td>\n      <td>0.00000-2.52098</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000004.wav</td>\n      <td>có rất nhiều yếu tố may rủi ở đây</td>\n      <td>0.00000-3.43298</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000005.wav</td>\n      <td>ai là chúa nói dối</td>\n      <td>0.00000-3.93698</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import re\nimport unicodedata\n\ndef clean_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower()\n    text = re.sub(r\"[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]\"\n                  r\"[^a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễ\"\n                  r\"ìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữ\"\n                  r\"ỳýỵỷỹđ ]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n    \ndef remove_punctuation(text):\n    if not isinstance(text, str):\n        return \"\"\n    # Bỏ hết ký tự không phải chữ cái, số hoặc khoảng trắng\n    text = re.sub(r'[^a-zA-ZÀ-ỹ0-9\\s]', '', text)\n    # Chuẩn hóa Unicode (tách dấu ra riêng)\n    text = unicodedata.normalize('NFD', text)\n    # Bỏ các ký tự dấu (combining marks)\n    text = ''.join(ch for ch in text if unicodedata.category(ch) != 'Mn')\n    # Bỏ khoảng trắng thừa và đưa về lowercase\n    text = text.lower().strip()\n    return text\n\ndf[\"caption\"] = df[\"caption\"].apply(remove_punctuation)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:49:45.073966Z","iopub.execute_input":"2025-08-27T08:49:45.074226Z","iopub.status.idle":"2025-08-27T08:49:45.513353Z","shell.execute_reply.started":"2025-08-27T08:49:45.074207Z","shell.execute_reply":"2025-08-27T08:49:45.512618Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                  file_name  \\\n0  FPTOpenSpeechData_Set001_V0.1_000001.wav   \n1  FPTOpenSpeechData_Set001_V0.1_000002.wav   \n2  FPTOpenSpeechData_Set001_V0.1_000003.wav   \n3  FPTOpenSpeechData_Set001_V0.1_000004.wav   \n4  FPTOpenSpeechData_Set001_V0.1_000005.wav   \n\n                                             caption              tag  \n0                                         cach đe đi  0.00000-1.27298  \n1  ho đa xet nghiem mau cho cheng nhung moi thu v...  0.00000-3.79298  \n2                           anh co the goi toi khong  0.00000-2.52098  \n3                  co rat nhieu yeu to may rui o đay  0.00000-3.43298  \n4                                 ai la chua noi doi  0.00000-3.93698  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>caption</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000001.wav</td>\n      <td>cach đe đi</td>\n      <td>0.00000-1.27298</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000002.wav</td>\n      <td>ho đa xet nghiem mau cho cheng nhung moi thu v...</td>\n      <td>0.00000-3.79298</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000003.wav</td>\n      <td>anh co the goi toi khong</td>\n      <td>0.00000-2.52098</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000004.wav</td>\n      <td>co rat nhieu yeu to may rui o đay</td>\n      <td>0.00000-3.43298</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000005.wav</td>\n      <td>ai la chua noi doi</td>\n      <td>0.00000-3.93698</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\naudio_folder = r'/kaggle/input/fosd-vietnamese/wav'\n\ndef find_audio_path(file_name):\n    path = os.path.join(audio_folder, file_name)\n    if os.path.exists(path):\n        return path\n    print(\"not find\",path)\n\ndf['audio_path'] = df['file_name'].apply(find_audio_path)\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:49:45.514313Z","iopub.execute_input":"2025-08-27T08:49:45.514677Z","iopub.status.idle":"2025-08-27T08:50:23.163147Z","shell.execute_reply.started":"2025-08-27T08:49:45.514653Z","shell.execute_reply":"2025-08-27T08:50:23.162329Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                  file_name  \\\n0  FPTOpenSpeechData_Set001_V0.1_000001.wav   \n1  FPTOpenSpeechData_Set001_V0.1_000002.wav   \n2  FPTOpenSpeechData_Set001_V0.1_000003.wav   \n3  FPTOpenSpeechData_Set001_V0.1_000004.wav   \n4  FPTOpenSpeechData_Set001_V0.1_000005.wav   \n\n                                             caption              tag  \\\n0                                         cach đe đi  0.00000-1.27298   \n1  ho đa xet nghiem mau cho cheng nhung moi thu v...  0.00000-3.79298   \n2                           anh co the goi toi khong  0.00000-2.52098   \n3                  co rat nhieu yeu to may rui o đay  0.00000-3.43298   \n4                                 ai la chua noi doi  0.00000-3.93698   \n\n                                          audio_path  \n0  /kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...  \n1  /kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...  \n2  /kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...  \n3  /kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...  \n4  /kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>caption</th>\n      <th>tag</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000001.wav</td>\n      <td>cach đe đi</td>\n      <td>0.00000-1.27298</td>\n      <td>/kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000002.wav</td>\n      <td>ho đa xet nghiem mau cho cheng nhung moi thu v...</td>\n      <td>0.00000-3.79298</td>\n      <td>/kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000003.wav</td>\n      <td>anh co the goi toi khong</td>\n      <td>0.00000-2.52098</td>\n      <td>/kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000004.wav</td>\n      <td>co rat nhieu yeu to may rui o đay</td>\n      <td>0.00000-3.43298</td>\n      <td>/kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FPTOpenSpeechData_Set001_V0.1_000005.wav</td>\n      <td>ai la chua noi doi</td>\n      <td>0.00000-3.93698</td>\n      <td>/kaggle/input/fosd-vietnamese/wav/FPTOpenSpeec...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Pre-Procesing Data","metadata":{}},{"cell_type":"markdown","source":"### Use model","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Audio, Dataset, DatasetDict\nfrom transformers import Wav2Vec2Processor\nfrom transformers import Wav2Vec2FeatureExtractor, AutoTokenizer\nimport torchaudio\n\n#load dataset\n# dataset = load_dataset(\n#     \"csv\",\n#     data_files={\n#         \"train\": \"train.csv\",\n#         \"validation\": \"val.csv\"\n#     }\n# )\n#dataset = load_dataset(\"csv\", data_files=\"all_data.csv\")\n\ndata = Dataset.from_pandas(df)\ndataset_split = data.train_test_split(test_size=0.1, seed=42)\ntrain_val_split = dataset_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n\nfinal_dataset = DatasetDict({\n    \"train\": dataset_split[\"train\"],\n    \"validation\": train_val_split[\"train\"],  # đây là val\n    \"test\": train_val_split[\"test\"]\n})\n\nprint(final_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:50:23.164728Z","iopub.execute_input":"2025-08-27T08:50:23.164968Z","iopub.status.idle":"2025-08-27T08:50:51.345600Z","shell.execute_reply.started":"2025-08-27T08:50:23.164950Z","shell.execute_reply":"2025-08-27T08:50:51.344820Z"}},"outputs":[{"name":"stderr","text":"2025-08-27 08:50:36.112464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756284636.345822      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756284636.414693      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['file_name', 'caption', 'tag', 'audio_path'],\n        num_rows: 23328\n    })\n    validation: Dataset({\n        features: ['file_name', 'caption', 'tag', 'audio_path'],\n        num_rows: 1296\n    })\n    test: Dataset({\n        features: ['file_name', 'caption', 'tag', 'audio_path'],\n        num_rows: 1297\n    })\n})\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# from torch.utils.data import Dataset\n# import torchaudio\n\n# class ASRDataset(Dataset):\n#     def __init__(self, audio_paths, transcripts, processor, sampling_rate=16000):\n#         self.audio_paths = audio_paths\n#         self.transcripts = transcripts\n#         self.processor = processor\n#         self.sampling_rate = sampling_rate\n\n#     def __getitem__(self, idx):\n#         speech_array, sr = torchaudio.load(self.audio_paths[idx])\n#         if sr != self.sampling_rate:\n#             speech_array = torchaudio.transforms.Resample(sr, self.sampling_rate)(speech_array)\n#         speech_array = speech_array.squeeze()\n\n#         # Xử lý audio & transcript cùng lúc\n#         inputs = self.processor(\n#             speech_array,\n#             sampling_rate=self.sampling_rate,\n#             text=self.transcripts[idx]\n#         )\n#         return {k: v.squeeze() for k, v in inputs.items()}\n\n#     def __len__(self):\n#         return len(self.audio_paths)\n\n\n# class TokenizerDataset(Dataset):\n#     def __init__(self, audio_paths, transcripts, processor, max_len=128):\n#         self.audio_paths = audio_paths\n#         self.transcripts = transcripts\n#         self.processor = processor\n#         self.max_len = max_len\n\n#     def __getitem__(self, idx):\n#         # ---- Load audio ----\n#         speech_array, sampling_rate = torchaudio.load(self.audio_paths[idx])\n#         speech_array = speech_array.squeeze().numpy()\n\n#         # Extract features (input cho encoder Wav2Vec2)\n#         inputs = self.processor.feature_extractor(\n#             speech_array,\n#             sampling_rate=sampling_rate,\n#             return_tensors=\"pt\"\n#         )\n\n#         # ---- Tokenize transcript ----\n#         labels = self.processor.tokenizer(\n#             self.transcripts[idx],\n#             padding=\"max_length\",\n#             truncation=True,\n#             max_length=self.max_len,\n#             return_tensors=\"pt\"\n#         ).input_ids.squeeze(0)\n\n#         return {\n#             \"input_values\": inputs.input_values.squeeze(0),  # (T, feature_dim)\n#             \"labels\": labels                                # (max_len,)\n#         }\n\n#     def __len__(self):\n#         return len(self.audio_paths)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:50:51.346723Z","iopub.execute_input":"2025-08-27T08:50:51.347611Z","iopub.status.idle":"2025-08-27T08:50:51.352116Z","shell.execute_reply.started":"2025-08-27T08:50:51.347580Z","shell.execute_reply":"2025-08-27T08:50:51.351165Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#load audio\ndataset = final_dataset.cast_column(\"audio_path\", Audio(sampling_rate=16000))\n\n#load tokenizer\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\ntext_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # hoặc \"facebook/bart-base\"\nvocab_size = len(text_tokenizer)\n\n\n# Pre-processing (extract feature + tokenize text)\ndef prepare_dataset(batch):\n    # Audio -> input_values\n    audio = batch[\"audio_path\"]\n    batch[\"input_values\"] = feature_extractor(audio[\"array\"], \n                                              sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n    \n    # Text -> labels\n    batch[\"labels\"] = text_tokenizer(batch[\"caption\"], \n                                     truncation=True, \n                                     padding=\"max_length\", \n                                     max_length=128).input_ids\n    return batch\n\ndataset = dataset.map(prepare_dataset, remove_columns=dataset[\"train\"].column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:50:51.352954Z","iopub.execute_input":"2025-08-27T08:50:51.353227Z","iopub.status.idle":"2025-08-27T08:56:23.208224Z","shell.execute_reply.started":"2025-08-27T08:50:51.353202Z","shell.execute_reply":"2025-08-27T08:56:23.207356Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d45563114be4aca90a9ea8fcd071080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17987e7ecab94849a467800c109ed229"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b66a2c893d547219c86f3759672fd3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f8462afbbb5454dae076913a2b91047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23328 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa3acd67d4c4d83ae8356a4fdd5e75d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1296 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e625347fc58948329fca1da1b3514be5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1297 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af987d8142a8446c88eb31775adcb627"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"#Khi làm ASR, đặc biệt với Wav2Vec2, Whisper, hay bất kỳ model CTC nào\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport torch\n\n@dataclass\nclass DataCollatorSeq2SeqWithPadding:\n    feature_extractor: Any\n    tokenizer: Any\n    padding: Union[bool, str] = True\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Pad audio\n        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n        batch = self.feature_extractor.pad(\n            input_features,\n            padding=self.padding,\n            return_tensors=\"pt\"\n        )\n\n        # Pad text labels\n        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n        labels_batch = self.tokenizer.pad(\n            label_features,\n            padding=self.padding,\n            return_tensors=\"pt\"\n        )\n\n        # Trong seq2seq vẫn thường mask pad thành -100 (giống CTC) để loss bỏ qua\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        batch[\"labels\"] = labels\n\n        return batch\n        \ndata_collator = DataCollatorSeq2SeqWithPadding(\n    feature_extractor=feature_extractor,\n    tokenizer=text_tokenizer,\n    padding=True )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:23.209184Z","iopub.execute_input":"2025-08-27T08:56:23.209891Z","iopub.status.idle":"2025-08-27T08:56:23.217222Z","shell.execute_reply.started":"2025-08-27T08:56:23.209869Z","shell.execute_reply":"2025-08-27T08:56:23.216649Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataset = dataset[\"train\"]\nval_dataset = dataset[\"validation\"]\ntest_dataset = dataset[\"test\"]\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=data_collator)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=data_collator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:23.218103Z","iopub.execute_input":"2025-08-27T08:56:23.218341Z","iopub.status.idle":"2025-08-27T08:56:23.234022Z","shell.execute_reply.started":"2025-08-27T08:56:23.218314Z","shell.execute_reply":"2025-08-27T08:56:23.233389Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Early Stopping","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, delta=0.0, path='best_model.pt'):\n        self.patience = patience\n        self.delta = delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.path = path\n\n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif val_loss > self.best_loss - self.delta:\n            self.counter += 1\n            print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n            self.counter = 0\n\n    def save_checkpoint(self, model):\n        torch.save(model.state_dict(), self.path)\n        print(f\"Saved best model with val_loss: {self.best_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:23.234716Z","iopub.execute_input":"2025-08-27T08:56:23.234970Z","iopub.status.idle":"2025-08-27T08:56:23.245691Z","shell.execute_reply.started":"2025-08-27T08:56:23.234952Z","shell.execute_reply":"2025-08-27T08:56:23.245034Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Encoder","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Model\nfrom transformers import get_scheduler\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:23.247482Z","iopub.execute_input":"2025-08-27T08:56:23.247652Z","iopub.status.idle":"2025-08-27T08:56:24.395063Z","shell.execute_reply.started":"2025-08-27T08:56:23.247638Z","shell.execute_reply":"2025-08-27T08:56:24.394246Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Wav2Vec2Encoder(nn.Module):\n    def __init__(self, model_name=\"facebook/wav2vec2-large-xlsr-53\"):\n        super().__init__()\n        self.encoder = Wav2Vec2Model.from_pretrained(model_name) #Wav2Vec2ForCTC\n\n    def forward(self, input_values, attention_mask=None):\n        outputs = self.encoder(input_values, attention_mask=attention_mask)\n        #return outputs.logits  \n        return outputs.last_hidden_state  # [B, T, hidden]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:24.395885Z","iopub.execute_input":"2025-08-27T08:56:24.396161Z","iopub.status.idle":"2025-08-27T08:56:24.400757Z","shell.execute_reply.started":"2025-08-27T08:56:24.396137Z","shell.execute_reply":"2025-08-27T08:56:24.400066Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Position Encoding","metadata":{}},{"cell_type":"markdown","source":"RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1 Compile with TORCH_USE_CUDA_DSA to enable device-side assertions.","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000, device='cpu'):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model, device=device)  # tạo trực tiếp trên device tránh lỗi trên\n        position = torch.arange(0, max_len, device=device).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, device=device) *\n                             (-torch.log(torch.tensor(10000.0, device=device)) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:24.401664Z","iopub.execute_input":"2025-08-27T08:56:24.402348Z","iopub.status.idle":"2025-08-27T08:56:24.417736Z","shell.execute_reply.started":"2025-08-27T08:56:24.402323Z","shell.execute_reply":"2025-08-27T08:56:24.416990Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Decoder","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n    def __init__(self, vocab_size, d_model=768, encoder_hidden_size=1024, num_layers=6, nhead=8, dim_feedforward=2048):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n        \n        self.enc_to_dec = nn.Linear(encoder_hidden_size, d_model)\n\n        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n        self.fc_out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, tgt_tokens, memory, tgt_mask=None, memory_mask=None):\n        # tgt_emb = self.embedding(tgt_tokens)  # [B, T, d_model]\n        # tgt_emb = self.pos_encoding(tgt_emb)\n        tgt_tokens_for_emb = tgt_tokens.clone()\n        tgt_tokens_for_emb[tgt_tokens_for_emb == -100] = 0  # giả sử pad_token_id = 0\n        #assert tgt_tokens_for_emb.max() < vocab_size\n\n        tgt_emb = self.embedding(tgt_tokens_for_emb)  # [B, T, d_model]\n        tgt_emb = self.pos_encoding(tgt_emb)\n\n        memory = self.enc_to_dec(memory) \n        \n        output = self.transformer_decoder(tgt_emb.transpose(0, 1), memory.transpose(0, 1),\n                                          tgt_mask=tgt_mask, memory_mask=memory_mask)\n        return self.fc_out(output.transpose(0, 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:24.418607Z","iopub.execute_input":"2025-08-27T08:56:24.419382Z","iopub.status.idle":"2025-08-27T08:56:24.432434Z","shell.execute_reply.started":"2025-08-27T08:56:24.419357Z","shell.execute_reply":"2025-08-27T08:56:24.431760Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# ASR Model (Encoder + Decoder)","metadata":{}},{"cell_type":"code","source":"class ASRModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, input_values, tgt_tokens, tgt_mask=None):\n        memory = self.encoder(input_values)\n        output = self.decoder(tgt_tokens, memory, tgt_mask=tgt_mask)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:24.433137Z","iopub.execute_input":"2025-08-27T08:56:24.433368Z","iopub.status.idle":"2025-08-27T08:56:24.446547Z","shell.execute_reply.started":"2025-08-27T08:56:24.433345Z","shell.execute_reply":"2025-08-27T08:56:24.445965Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, criterion, device, lr_scheduler):\n    model.train()\n    total_loss, total_samples = 0, 0\n    loop = tqdm(train_loader, leave=True)\n    #loop = tqdm(val_loader, total=len(val_loader), leave=True, ncols=100, desc=\"Trainning\")\n    for batch in loop:\n        input_values = batch[\"input_values\"].to(device)\n        #labels = batch[\"labels\"].to(device)\n        #decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n        labels = batch[\"labels\"][:, 1:].to(device)  \n        decoder_input_ids = batch[\"labels\"][:, :-1].to(device)\n\n        optimizer.zero_grad()\n        logits = model(input_values, decoder_input_ids)\n        loss = criterion(logits.reshape(-1, logits.size(-1)),labels.reshape(-1))\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n        total_loss += loss.item() * input_values.size(0)\n        total_samples += input_values.size(0)\n        #loop.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / total_samples\n    return avg_loss\n\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    total_loss, total_samples = 0, 0\n    with torch.no_grad():\n        loop = tqdm(val_loader, leave=True)\n        #loop = tqdm(val_loader, total=len(val_loader), leave=True, ncols=100, desc=\"Validating\")\n        for batch in loop:\n            input_values = batch[\"input_values\"].to(device)\n            decoder_input_ids = batch[\"labels\"][:, :-1].to(device) \n            labels = batch[\"labels\"].to(device)\n    \n            outputs = model(input_values, decoder_input_ids)\n            loss = outputs.loss\n\n            total_loss += loss.item() * input_values.size(0)\n            total_samples += input_values.size(0)\n            #loop.set_postfix(loss=loss.item())\n            \n    avg_loss = total_loss / total_samples\n    return avg_loss ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:56:24.447463Z","iopub.execute_input":"2025-08-27T08:56:24.447718Z","iopub.status.idle":"2025-08-27T08:56:24.460013Z","shell.execute_reply.started":"2025-08-27T08:56:24.447692Z","shell.execute_reply":"2025-08-27T08:56:24.459201Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"encoder = Wav2Vec2Encoder()\ndecoder = TransformerDecoder(vocab_size)\nmodel = ASRModel(encoder, decoder)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = nn.DataParallel(model).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n# KHi traning asr end to end thì cái criterion được tích hợp sẵn trong label của hugging face nên ta không cần \n# phải gọi thủ công nữa\n\n\nnum_epochs = 30\nnum_training_steps = num_epochs * len(train_loader)\nlr_scheduler = get_scheduler(\n    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\nearly_stopper = EarlyStopping(patience=3)\n\nfor epoch in range(num_epochs):\n    train_loss = train_one_epoch(model, train_loader,  optimizer, criterion, device, lr_scheduler)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n\n    val_loss = validate(model, val_loader, criterion, device)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}\")\n    \n    early_stopper(val_loss, model)\n    if early_stopper.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null}]}